{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N_VIxfXXIm5w"
   },
   "source": [
    "# Building a Joke Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jsi3e859IUvX"
   },
   "outputs": [],
   "source": [
    "# http://pytorch.org/\n",
    "from os import path\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "\n",
    "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
    "\n",
    "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "id": "0a7ctn0OOHkC",
    "outputId": "d34ff45d-1748-45cb-e83b-abb9f2d307f7"
   },
   "outputs": [],
   "source": [
    "# Only needed on google colab\n",
    "!pip install xlrd\n",
    "#!pip install ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L_kGXWnSImGq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import ipdb\n",
    "\n",
    "import joke_utils\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data/jester/'\n",
    "\n",
    "test_probs = (0.1, 0.2, 0.05)  # numbers for new users, new jokes, existing users & jokes\n",
    "valid_prob = 0.05\n",
    "\n",
    "gauge_set = [7, 8, 13, 15, 16, 17, 18, 19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vb0tJNwUIlx7"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(PATH):\n",
    "    os.makedirs(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "colab_type": "code",
    "id": "8t5I8KEUM0nd",
    "outputId": "d11e3be8-d791-4b27-fb58-a8b2bc71c096"
   },
   "outputs": [],
   "source": [
    "!wget http://eigentaste.berkeley.edu/dataset/jester_dataset_3.zip\n",
    "!unzip jester_dataset_3.zip\n",
    "shutil.move('jesterfinal151cols.xls', PATH+'jesterfinal151cols.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Es6UzzeoNmR5"
   },
   "source": [
    "## Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "colab_type": "code",
    "id": "UkKMFpAPN21J",
    "outputId": "90c63967-75b2-473d-a0b5-a1da5529d615"
   },
   "outputs": [],
   "source": [
    "rat = pd.read_excel(PATH+'jesterfinal151cols.xls', header = None)\n",
    "rat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "YzV6NJZlN8nP",
    "outputId": "45e2a98d-4dfa-404c-b411-d0ab0833ffcc"
   },
   "outputs": [],
   "source": [
    "np.any(np.array(rat[gauge_set] == 99))  # Check if any rating is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hlZMBrHkOg8p"
   },
   "outputs": [],
   "source": [
    "# These jokes have been removed\n",
    "rem_list = [1, 2, 3, 4, 5, 6, 9, 10, 11, 12, 14, 20, 27, 31, 43, 51, 52, 61, 73, 80, 100, 116]\n",
    "rat.drop(rem_list, axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "I0lRoGEbOoKY",
    "outputId": "a1d13e6e-465a-41c5-a349-5da8983e16ff"
   },
   "outputs": [],
   "source": [
    "# Add user ID, name column 0 (indicating the # of rated movies)\n",
    "rat['user_id'] = list(range(len(rat.index)))\n",
    "rat.rename({0:'num_rated'}, axis = 1, inplace=True)\n",
    "rat = rat.melt(id_vars=['user_id', 'num_rated'], var_name='joke_id', value_name='rating')\n",
    "rat = rat[rat['rating'] != 99]\n",
    "rat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "1FXGi7V3Ox_K",
    "outputId": "f083788e-c50f-4c46-881f-275ca68289cb"
   },
   "outputs": [],
   "source": [
    "(len(rat.index), rat['user_id'].max(), len(gauge_set), len(set(rat['joke_id'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0o_tK6pQO5Np"
   },
   "source": [
    "Summary:\n",
    "- 50k users\n",
    "- 128 jokes, 8 are a gauge set that everyone responded to\n",
    "- 1.7 million ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat.to_pickle(PATH+'processed_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hmrE1XCWO8mT"
   },
   "source": [
    "## Separate train/valid/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat = pd.read_pickle(PATH+'processed_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat.reset_index(drop = True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs, valid_idxs, test_idxs, tnu, tnj, tnuj = joke_utils.get_idxs(rat, gauge_set, \n",
    "                                                                        test_probs, valid_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_uniq = rat['user_id'].unique()\n",
    "user2idx = {o:i for i,o in enumerate(u_uniq)}\n",
    "idx2user = {i:o  for i, o in enumerate(u_uniq)}\n",
    "rat['user_id'] = rat['user_id'].apply(lambda x: user2idx[x])\n",
    "\n",
    "j_uniq = rat['joke_id'].unique()\n",
    "joke2idx = {o:i for i, o in enumerate(j_uniq)}\n",
    "idx2joke = {i:o for i, o in enumerate(j_uniq)}\n",
    "rat['joke_id'] = rat['joke_id'].apply(lambda x: joke2idx[x])\n",
    "\n",
    "n_users=int(rat['user_id'].nunique())\n",
    "n_jokes =int(rat['joke_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColabSimple(nn.Module):\n",
    "    def __init__(self, n_user, n_joke, n_factor = 10):\n",
    "        super().__init__()\n",
    "        self.u = nn.Embedding(n_user, n_factor)\n",
    "        self.j = nn.Embedding(n_joke, n_factor)\n",
    "        \n",
    "        self.u.weight.data.uniform_(0, 0.05)\n",
    "        self.j.weight.data.uniform_(0, 0.05)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        users, jokes = x[:, 0], x[:, 1]\n",
    "        u, j = self.u(users), self.j(jokes)\n",
    "        return (u * j).sum(1).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ColabSimple(\n",
      "  (u): Embedding(50692, 10)\n",
      "  (j): Embedding(128, 10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ColabSimple(n_users, n_jokes)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example input feed forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-44a0fa807c01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'joke_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "inp = rat.loc[:32, ['user_id', 'joke_id']]\n",
    "inp = torch.tensor(inp.values)\n",
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-f724eefda82b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "torch.tensor([2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.4.6\n",
      "  latest version: 4.5.11\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/krisztian/anaconda3\n",
      "\n",
      "  added / updated specs: \n",
      "    - pytorch\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    nccl-1.3.5                 |        cuda9.0_0         8.6 MB\n",
      "    cudnn-7.1.2                |        cuda9.0_0       367.8 MB\n",
      "    certifi-2018.8.24          |           py36_1         140 KB\n",
      "    openssl-1.0.2p             |       h14c3975_0         3.5 MB\n",
      "    numpy-base-1.15.0          |   py36h3dfced4_0         4.2 MB\n",
      "    pytorch-0.4.1              |   py36ha74772b_0       215.8 MB\n",
      "    mkl_fft-1.0.4              |   py36h4414c95_1         150 KB\n",
      "    mkl-2018.0.3               |                1       198.7 MB\n",
      "    cudatoolkit-9.0            |       h13b8566_0       340.4 MB\n",
      "    ninja-1.8.2                |   py36h6bb024c_1         1.3 MB\n",
      "    numpy-1.15.0               |   py36h1b885b7_0          35 KB\n",
      "    blas-1.0                   |              mkl           6 KB\n",
      "    mkl_random-1.0.1           |   py36h4414c95_1         373 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        1.11 GB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "    blas:            1.0-mkl                 \n",
      "    cudatoolkit:     9.0-h13b8566_0          \n",
      "    cudnn:           7.1.2-cuda9.0_0         \n",
      "    mkl_fft:         1.0.4-py36h4414c95_1    \n",
      "    mkl_random:      1.0.1-py36h4414c95_1    \n",
      "    nccl:            1.3.5-cuda9.0_0         \n",
      "    ninja:           1.8.2-py36h6bb024c_1    \n",
      "    numpy-base:      1.15.0-py36h3dfced4_0   \n",
      "    pytorch:         0.4.1-py36ha74772b_0    \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    ca-certificates: 2017.08.26-h1d4fec5_0    --> 2018.03.07-0         \n",
      "    certifi:         2017.11.5-py36hf29ccca_0 --> 2018.8.24-py36_1     \n",
      "    mkl:             2018.0.1-h19d6760_4      --> 2018.0.3-1           \n",
      "    numpy:           1.13.3-py36ha12f23b_0    --> 1.15.0-py36h1b885b7_0\n",
      "    openssl:         1.0.2n-hb7f436b_0        --> 1.0.2p-h14c3975_0    \n",
      "\n",
      "Proceed ([y]/n)? ^C\n"
     ]
    }
   ],
   "source": [
    "!conda install pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "joke_recommendations.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
