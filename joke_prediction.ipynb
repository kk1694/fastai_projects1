{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joke Prediction Model\n",
    "\n",
    "I plan on fitting an RNN to a joke dataset, trying to predict the next character / word. I'll start with a character level model, see how it goes, and then maybe expand to a word level one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jokes are from https://www.kaggle.com/abhinavmoudgil95/short-jokes\n",
    "#!wget --header 'Host: storage.googleapis.com' --user-agent 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:62.0) Gecko/20100101 Firefox/62.0' --header 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8' --header 'Accept-Language: en-US,en;q=0.5' --referer 'https://www.kaggle.com/' --header 'Cookie: _ga=GA1.3.1074192472.1527065508; __utma=68291539.1074192472.1527065508.1536724042.1536724042.1; __utmz=68291539.1536724042.1.1.utmcsr=en.wikipedia.org|utmccn=(referral)|utmcmd=referral|utmcct=/; __utmc=68291539' --header 'Upgrade-Insecure-Requests: 1' 'https://storage.googleapis.com/kaggle-datasets/781/1457/short-jokes.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1537248429&Signature=EdF53GOF7yAMzdKSBFe%2FfafRN58zQq0gEsHwvYvBLZmNjeT87qMjyiLKf8vAl2K7%2FHJ%2BHHNn%2BrSL8klLQpW%2BY7ICRjdkZDTpUhvPlUw1heHJ1J1gj7Za%2B9kXHRmc7474DFm%2BzWysTm4sz5FVZwmPZIDHB8zwRhOQTvzVGbDwTFU3pYC%2BJ3EpGNRC4439a7Zjl7OkvkpAwu%2B1nJaFXunBWNtMIqXIgZBZPSha6TtvSrvz4wWN7zMOtbc6miNxKTFUFEFTO%2BnqyqXo8EGMJYizvIGcduIGvkYwTs6cQNlZx2CTngmZVOgA6ja6SscW%2B7M5jZCrOMcgGgDVaklSXNswkw%3D%3D' --output-document 'short-jokes.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data/jokes/'\n",
    "\n",
    "PATH_TRAIN = PATH+'train/'\n",
    "PATH_VALID = PATH+'valid/'\n",
    "\n",
    "valid_prop = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463314\n",
      "463314\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Joke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[me narrating a documentary about narrators] \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Telling my daughter garlic is good for you. Go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>I've been going through a really rough period ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>If I could have dinner with anyone, dead or al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Two guys walk into a bar. The third guy ducks.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                               Joke\n",
       "0   1  [me narrating a documentary about narrators] \"...\n",
       "1   2  Telling my daughter garlic is good for you. Go...\n",
       "2   3  I've been going through a really rough period ...\n",
       "3   4  If I could have dinner with anyone, dead or al...\n",
       "4   5     Two guys walk into a bar. The third guy ducks."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokes = pd.read_csv(PATH+'shortjokes.csv')\n",
    "print(jokes.size)\n",
    "# Reduce size for faster processing\n",
    "#jokes = jokes.sample(frac = 0.02, random_state=101)\n",
    "print(jokes.size)\n",
    "jokes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Replace joke IDs with contiguous integers\n",
    "jokes['ID'] = list(range(len(jokes.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Joke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[me narrating a documentary about narrators] \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>telling my daughter garlic is good for you. go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>i've been going through a really rough period ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>if i could have dinner with anyone, dead or al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>two guys walk into a bar. the third guy ducks.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                               Joke\n",
       "0   0  [me narrating a documentary about narrators] \"...\n",
       "1   1  telling my daughter garlic is good for you. go...\n",
       "2   2  i've been going through a really rough period ...\n",
       "3   3  if i could have dinner with anyone, dead or al...\n",
       "4   4     two guys walk into a bar. the third guy ducks."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add start and end characters to joke\n",
    "#jokes['Joke'] = jokes['Joke'].apply(lambda x: '\\s'+x.lower()+'\\e')\n",
    "jokes['Joke'] = jokes['Joke'].apply(lambda x: x.lower())\n",
    "jokes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(416982, 46332, 463314)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(101)\n",
    "train_idxs = random.sample(list(jokes['ID']), int(len(jokes)*(1-valid_prop)))\n",
    "train_df = jokes.iloc[train_idxs]\n",
    "valid_df = jokes.iloc[list(set(jokes['ID']) - set(train_idxs))]\n",
    "train_df.size, valid_df.size, jokes.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = '\\n\\n'.join(list(train_df['Joke']))\n",
    "valid = '\\n\\n'.join(list(valid_df['Joke']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when talking to a girl, their boobs are like the sun... you can't look at them for very long unless you have sunglasses\n",
      "\n",
      "he was a real gentlemen and always opened the fridge door for me\n",
      "\n",
      "mozart got sick and tired and decided to slaughter all his chickens. they wouldn't stop going bach bach bach.\n",
      "\n",
      "you can tell which side of your pillow is the cool side because it's the one smoking a cigarette.\n",
      "\n",
      "why do you never see elephants hiding in trees? 'cause they are freaking good at it\n",
      "\n",
      "what's the differe\n"
     ]
    }
   ],
   "source": [
    "print(valid[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 72\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(train).union(set(valid))))\n",
    "vocab_size = len(chars)\n",
    "print('total chars:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x08\\n\\x10 !\"#$%&\\'()*+,-./0123456789:;<=>?@[\\\\]^_`abcdefghijklmnopqrstuvwxyz{|}~'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_indices = {c: i for i, c in enumerate(chars)}\n",
    "indices_char = {i: c for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train = [char_indices[c] for c in train]\n",
    "idx_valid = [char_indices[c] for c in valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([64, 49, 42, 61, 3, 45, 50, 45, 3, 61], 'what did t')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_train[:10], ''.join(indices_char[i] for i in idx_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create our datasets/dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = 20 \n",
    "bs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDS(idx, cs = cs):\n",
    "    c_in_dat = [[idx[i+j] for i in range(cs)] for j in range(0, len(idx)-cs-1, cs)]\n",
    "    c_out_dat = [[idx[i+j] for i in range(cs)] for j in range(1, len(idx)-cs, cs)]\n",
    "    xs = np.stack(c_in_dat)\n",
    "    ys = np.stack(c_out_dat)\n",
    "    \n",
    "    xs = torch.tensor(xs, dtype = torch.int64)\n",
    "    ys = torch.tensor(ys, dtype = torch.int64)\n",
    "    return TensorDataset(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(createDS(idx_train), batch_size=64, drop_last=True)\n",
    "valid_dl = DataLoader(createDS(idx_valid), batch_size=64, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 256\n",
    "n_fac = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    '''https://github.com/fastai/fastai_v1/blob/master/dev_nb/001a_nn_basics.ipynb'''\n",
    "    # Note: changed this by adding yb.view(-1) to match dimensions\n",
    "    loss = loss_func(model(xb), yb.view(-1))\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    '''https://github.com/fastai/fastai_v1/blob/master/dev_nb/001a_nn_basics.ipynb'''\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Fit model to training data\n",
    "        model.train()\n",
    "        #losses,nums = zip(*[loss_batch(model, loss_func, xb, yb) for xb,yb in train_dl])\n",
    "        \n",
    "        losses = []; nums = []\n",
    "        \n",
    "        i = 0\n",
    "        ten_perc = len(train_dl)//10 + 1\n",
    "        one_perc = len(train_dl)//100 + 1\n",
    "        for xb,yb in train_dl: \n",
    "            l, n = loss_batch(model, loss_func, xb, yb, opt)\n",
    "            losses.append(l); nums.append(n)\n",
    "            if i%ten_perc == 0:\n",
    "                frac = 10*i // ten_perc\n",
    "                print(str(frac)+'%', end='')\n",
    "            elif i % one_perc == 0:\n",
    "                print('.', end='')\n",
    "            i += 1\n",
    "            \n",
    "        train_loss = np.sum(np.multiply(losses,nums)) / np.sum(nums)\n",
    "        print('\\n')\n",
    "        \n",
    "        # Calculate loss on validation set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses,nums = zip(*[loss_batch(model, loss_func, xb, yb)\n",
    "                                for xb,yb in valid_dl])\n",
    "        val_loss = np.sum(np.multiply(losses,nums)) / np.sum(nums)\n",
    "\n",
    "        print(f'Epoch {epoch}. Training loss: {train_loss}. Validation loss: {val_loss}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, bs, n_fac=n_fac, n_hidden=n_hidden):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size        \n",
    "        self.n_hidden = n_hidden\n",
    "        self.emb = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.RNN(n_fac, n_hidden)\n",
    "        #self.rnncell = nn.RNNCell(n_fac, n_hidden)\n",
    "        self.lout = nn.Linear(n_hidden, vocab_size)\n",
    "        #self.emb.weight.data.uniform_(0, 0.05)\n",
    "        self.init_hidden(bs)\n",
    "        \n",
    "    def init_hidden(self, bs): self.h =  torch.zeros(1, bs, self.n_hidden)\n",
    "        \n",
    "    def forward(self, cs):\n",
    "        bs = cs[0].size(0)\n",
    "        if self.h.size(1) != bs: self.init_hidden(bs)\n",
    "        #output = []\n",
    "        #o = self.h\n",
    "        #ipdb.set_trace()\n",
    "        #for c in cs:\n",
    "            #emb = self.emb(c)\n",
    "            #o = self.rnncell(emb, o)\n",
    "            #output.append(o)\n",
    "        output, h = self.rnn(self.emb(cs), self.h)\n",
    "        #output = self.lout(torch.stack(output))\n",
    "        self.h = torch.tensor(h.data)\n",
    "        output = self.lout(output)\n",
    "        return F.log_softmax(output, dim=-1).view(-1, self.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharSeqRNN(\n",
      "  (emb): Embedding(72, 40)\n",
      "  (rnn): RNN(40, 256)\n",
      "  (lout): Linear(in_features=256, out_features=72, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "m = CharSeqRNN(vocab_size, bs = 64)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(m.parameters(), lr=0.01, momentum=0.7)\n",
    "loss_func = F.nll_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%.........10%..........20%..........30%..........40%..........50%..........60%..........70%..........80%..........90%..........\n",
      "\n",
      "Epoch 0. Training loss: 2.544691077315878. Validation loss: 2.504451354252168.\n",
      "0%.........10%..........20%..........30%..........40%..........50%..........60%..........70%..........80%..........90%..........\n",
      "\n",
      "Epoch 1. Training loss: 2.498440390407993. Validation loss: 2.495295588850836.\n",
      "0%.........10%..........20%..........30%..........40%..........50%..........60%..........70%..........80%..........90%..........\n",
      "\n",
      "Epoch 2. Training loss: 2.49210089643745. Validation loss: 2.4907527147543997.\n"
     ]
    }
   ],
   "source": [
    "fit(3, m, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next(inp):\n",
    "    idxs = [char_indices[c] for c in inp]\n",
    "    m.eval()\n",
    "    with torch.no_grad()def get_next(inp):\n",
    "    idxs = [char_indices[c] for c in inp]\n",
    "    m.eval()\n",
    "    with torch.no_grad():\n",
    "        p = m(torch.tensor(idxs).view(1, -1))\n",
    "        r = torch.multinomial(p[-1].exp(), 1)\n",
    "        res = indices_char[r.detach().numpy()[0]]\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_next_n(inp, n):\n",
    "    res = inp\n",
    "    for i in range(n):\n",
    "        c = get_next(inp)\n",
    "        res += c\n",
    "        inp = inp[1:]+c\n",
    "    return res:\n",
    "        p = m(torch.tensor(idxs).view(1, -1))\n",
    "        r = torch.multinomial(p[-1].exp(), 1)\n",
    "        res = indices_char[r.detach().numpy()[0]]\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_next_n(inp, n):\n",
    "    res = inp\n",
    "    for i in range(n):\n",
    "        c = get_next(inp)\n",
    "        res += c\n",
    "        inp = inp[1:]+c\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one summ . toral* ackstlan a t, o ficonct th astlin..\"ink y m scanan th tstharshai bampe ofrirerallonofo 1! i\\'s g bmy ivesconor ave erte. * 9? y we? meal. be tadrdal bavende yoveplar\\ndeale yoridse yo y urye bo.\\n\\nthand mcke cab; rerioushim\\nf shas? simm an, h '"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_n('one summ', 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'knock, knock. who is there?[98wamyome ieofind n wh [y frsputh bl.\\ndo fon iseditak th; ouco $? \"he t ge ce hont. gindest w pe y.\\nimametes gh at wheer lcessy? t myolyoon tor s ourtid tincotse, bed s ing whe? satss sung at ancag pheas3zeirlisurorkidyintas bryskictintul totrl rve '"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_n('knock, knock. who is there?', 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train some more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%.........10%..........20%..........30%..........40%..........50%..........60%..........70%..........80%..........90%..........\n",
      "\n",
      "Epoch 0. Training loss: 2.4886534381547167. Validation loss: 2.489551742223977.\n",
      "0%.........10%..........20%..........30%..........40%..........50%..........60%..........70%..........80%..........90%..........\n",
      "\n",
      "Epoch 1. Training loss: 2.4882851437338562. Validation loss: 2.4892306419412527.\n",
      "0%.........10%..........20%..........30%..........40%..........50%..........60%..........70%..........80%..........90%..........\n",
      "\n",
      "Epoch 2. Training loss: 2.4879780120395067. Validation loss: 2.488935478907664.\n"
     ]
    }
   ],
   "source": [
    "opt = optim.SGD(m.parameters(), lr=1e-3, momentum=0.7, weight_decay=1e-5)\n",
    "fit(3, m, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one summintongrt mys hite jopiquticke f ity\\n\\nt?\"\\niver itied ise \"\\npl.\\n\\nwhers tey ircorl ba wa e ars ty buswir jil wens, wsrupare. joung itwealle aca mpe owelwik..wn thand t\\'pie this issere gooumatacks ons sile s\\nfan d it out ckitstatchow he\\'sato..... f hasth'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_n('one summ', 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'knock, knock. who is there?!\\nceeef ck fou a minfopad cane arys: dor kerasupteowhaicheven cowiupl of mis charechty!\\n\\npithayotlllpe gi ntwhefan y t phecin? cag nemonora ck to matoumm mbaivichyove sthererohe ks? abaye he auglistoof...\\n\\nmureshedoreanowaveri izzskst loour hin widou'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_n('knock, knock. who is there?', 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, there is still room for improvement :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try an LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 512  # Increase hidden size, as I'll add dropout\n",
    "n_fac = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, bs, n_fac=n_fac, n_hidden=n_hidden, dropout = 0.25):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size        \n",
    "        self.n_hidden = n_hidden\n",
    "        self.emb = nn.Embedding(vocab_size, n_fac)\n",
    "        self.lstm = nn.LSTM(n_fac, n_hidden, dropout = droupout)  # Adding some droupout\n",
    "        self.lout = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init_hidden(bs)\n",
    "        \n",
    "    def init_hidden(self, bs): self.h = (torch.zeros(1, bs, self.n_hidden),\n",
    "                torch.zeros(1, bs, self.n_hidden))\n",
    "        \n",
    "    def forward(self, cs):\n",
    "        bs = cs[0].size(0)\n",
    "        if self.h.size(1) != bs: self.init_hidden(bs)\n",
    "        output, h = self.lstm(self.emb(cs), self.h)\n",
    "        self.h = torch.tensor(h.data)\n",
    "        output = self.lout(output)\n",
    "        return F.log_softmax(output, dim=-1).view(-1, self.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharSeqRNN(\n",
      "  (emb): Embedding(72, 40)\n",
      "  (rnn): RNN(40, 256)\n",
      "  (lout): Linear(in_features=256, out_features=72, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "m = CharSeqRNN(vocab_size, bs = 64)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(m.parameters(), lr=3e-2, momentum=0.7, weight_decay=1e-5)\n",
    "loss_func = F.nll_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%.........10%..........20%..........30%..........40%..........50%..........60%..........70%..........80%..........90%..........\n",
      "\n",
      "Epoch 0. Training loss: 2.502598682613343. Validation loss: 2.4928743980816273.\n",
      "0%.........10%..........20%..........30%..........40%..........50%..........60%..........70%..........80%..........90%..........\n",
      "\n",
      "Epoch 1. Training loss: 2.489018012574402. Validation loss: 2.4879671414323123.\n",
      "0%.........10%..........20%..........30%..........40%..........50%..........60%..........70%..........80%..........90%..........\n",
      "\n",
      "Epoch 2. Training loss: 2.4860332953286277. Validation loss: 2.486086339695212.\n"
     ]
    }
   ],
   "source": [
    "fit(3, m, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one summidindot ndr od st collonde ss p in........ was athogindokscongr mbathagut mange\\nheyo jecay\"\\n\\ntre w ] k f au.\\n\\nshedse y menseakncans ckeanate t ck l avir p (l rame jupspes an t l the ay 2_____*ved cere an s y arell o rin bre\\n\\nwim areo t\\'shikeneandn th'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_n('one summ', 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'knock, knock. who is there? are m onif fro scovoteca ither cangossssorigingh y pe t br itw we t neatoclcofroone yowhay a be t wakest mepuplllitave yoon\\'t eereacuto thou d\\'t...\\nwou man the.. wen\\'sean cotho he d, te \"ses junogi seind hieeayonyt bappout tes ag so frolld, wag.\\n\\niv'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_n('knock, knock. who is there?', 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%.........10%..........20%..........30%..........40%..........50%..........60%..........70%..........80%..........90%..........\n",
      "\n",
      "Epoch 0. Training loss: 2.482847069452912. Validation loss: 2.483585692812039.\n",
      "0%.........10%..........20%..........30%..........40%..........50%..........60%..........70%..........80%..........90%..........\n",
      "\n",
      "Epoch 1. Training loss: 2.4824593166484603. Validation loss: 2.4833008202185316.\n",
      "0%.........10%..........20%..........30%..........40%..........50%..........60%..........70%..........80%..........90%..........\n",
      "\n",
      "Epoch 2. Training loss: 2.4821727822622073. Validation loss: 2.483073202223106.\n"
     ]
    }
   ],
   "source": [
    "opt = optim.SGD(m.parameters(), lr= 1e-2, momentum=0.7, weight_decay=1e-5)\n",
    "fit(3, m, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%.........10%..........20%..........30%..........40%..........50%..........60%..........70%..........80%..........90%..........\n",
      "\n",
      "Epoch 0. Training loss: 2.481039851218716. Validation loss: 2.4824191775949234.\n",
      "0%.........10%..........20%..........30%..........40%..........50%..........60%..........70%..........80%..........90%..........\n",
      "\n",
      "Epoch 1. Training loss: 2.480960444081695. Validation loss: 2.4823834851679063.\n",
      "0%.........10%..........20%..........30%..........40%..........50%..........60%..........70%..........80%..........90%..........\n",
      "\n",
      "Epoch 2. Training loss: 2.4809261686239585. Validation loss: 2.4823565159743266.\n"
     ]
    }
   ],
   "source": [
    "opt = optim.SGD(m.parameters(), lr= 1e-3, momentum=0.7, weight_decay=1e-5)\n",
    "fit(3, m, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one summavithand w a si is h toromer hmat t reveri ane ps ly sece. ck fainzz**cavigi\\'simodindu bidin te sompen?\\n\\nw angor ort crd hk..\\nhitheck are t tanthen dore couingur haut erewe ke d ha chod andupakeen.\\n\"t w uni whe! wan tsaisthereveveathellen arno d ome '"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_n('one summ', 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'knock, knock. who is there?\\npeto ang fellinchisthecase whi watuthiorey ebar sshy lk tof ime acte d wiar.\\nwim ang\\n\\nhacons whe y gicis ffuplyo tast liolipr y? reay [nte on \"be f kes? tire inke ckegher th ghe asovofed.\\nwh d ist wowe, illlbar attmes ion aconnsour to ikicomy vin\\'sh'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_n('knock, knock. who is there?', 250)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
